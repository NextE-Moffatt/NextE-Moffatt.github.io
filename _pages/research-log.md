---
permalink: /research-log
title: "研究日志"
excerpt: "记录日常研究工作和想法"
author_profile: true
toc: true
toc_sticky: true
---

# 2024年4月
## 2024-04-11
### 多模态检索
- 调研了相关算法
- 基本框架 

[参考链接1](https://blog.langchain.dev/semi-structured-multi-modal-rag/)

[参考链接2](https://www.analyticsvidhya.com/blog/2024/09/guide-to-building-multimodal-rag-systems/)

![多模态检索框架](images/multimodal-retrieval-framework.png)
- 流程图
![流程图](images/multimodal-retrieval-workflow.png)
- 调研了多模态检索的三种主要方案：
  - 方案1：基于多模态向量检索
    - 将图表和文本分别编码为向量
    - 使用向量相似度进行检索
    - 优点：实现简单，检索速度快
    - 缺点：可能丢失细粒度语义信息
  
  - 方案2：基于跨模态对齐
    - 学习图文之间的对齐关系
    - 使用注意力机制进行特征融合
    - 优点：可以捕获更细粒度的语义对应
    - 缺点：计算开销较大
    
  - 方案3：基于混合检索策略
    - 结合向量检索和跨模态对齐
    - 先粗排后精排的两阶段方案
    - 优点：平衡了效率和效果
    - 缺点：系统复杂度增加

- 分析了各方案的适用场景：
  - 方案1适合大规模快速检索
  - 方案2适合对检索精度要求高的场景
  - 方案3适合实际应用部署(采用该方案)

- 运行了基线模型
- 实验结果：
  - BLEU: xx.xx
  - METEOR: xx.xx
- 发现的问题：
  - 在处理长句子时性能下降明显
  - 视觉特征提取可能需要优化

### 周会汇报准备
- 整理了本周实验结果
- 准备了演示 demo
- 需要补充：
  - [ ] 添加定量分析
  - [ ] 准备失败案例分析

# 备忘录
## 待办事项
- [ ] 准备下周组会报告
- [ ] 完成实验代码重构
- [ ] 整理文献综述

## 研究想法
- 想法1：将强化学习应用到对话生成中
- 想法2：探索多模态预训练的新方法

## 有用的资源
- 数据集列表：...
- 常用工具：...
- 参考论文：... 

# 2025-04-14
### deepseek-r1显存占用

| 模块 | 显存占用估算（FP8） | 说明 |
|------|-------------------|------|
| 🧠 模型权重本体 | ~640 GB | 672B 参数 × 1 Byte（FP8 编码） + 结构 buffer |
| 🗃️ 上下文缓存（prompt embeddings） | ~140 GB | 长 prompt 或系统提示信息存储区 |
| 📊 KV Cache（多轮对话缓存） | 200–300 GB | 注意力缓存，随 token 数增长；影响最大 |
| ✅ 总计推理负载 | ~1.0–1.1 TB | 真实推理状态下所需 GPU 显存峰值 |

### 实测数据

| 部署规模 | 并发数 | 速度 |
|---------|--------|------|
| 单台8卡H200 | 128 | 11 tokens/s |
| 单台8卡H200 | 256 | 7 tokens/s |
| 双台8卡H200 | 300-350 | 7-10 tokens/s |

支持用户总数约为：2000人


# 2025-4-15
### 监控助手
- 完成Mac电脑上调用摄像头和录音
- 尝试调用本地模型

# 2025-4-16
### 服务器空间不足解决方案
- 扩容
```
sudo lvextend -L +20G /dev/ubuntu-vg/ubuntu-lv
sudo resize2fs /dev/ubuntu-vg/ubuntu-lv
```

# 2025-5-16~2025-6-16
### 天迹原型搭建
- 检测图
- 完成自动生成检测内容功能和视频流实时检测
![实时检测图](images/tianji.png)

# 2025-7-7
服务器连接本地代理
```
export https_proxy=http://192.168.0.175:7890 http_proxy=http://192.168.0.175:7890 all_proxy=socks5://192.168.0.175:7890
```
# 2025-8-15
服务器转发到本地屏幕
```
/opt/TurboVNC/bin/vncserver :1
### 本地端口设置
192.169.0.200:5901
```
# 2025-8-22

# 大模型基础面
## 大模型基础面
### 目前主流的开源模型体系有哪些？
* gpt: decoder-only，自回归，单向从左到右，适合生成任务
* bert：encoder-only，自编码，双向，通过遮蔽和下一句预测来是模型充分理解上下文
* xlnet：自回归和自编码的结合，排列语言建模，通过排列捕捉更丰富的上下文信息提高理解能力
* Roberta：移除下一句预测，使用动态的遮蔽语言模型，使用更大规模的数据
* T5: encoder-decoder，填空式
### prefix Decoder 和 causal Decoder 和 Encoder-Decoder 区别是什么？
- Prefix Decoder：前缀部分可见，生成部分自回归（DeepSeek、ChatGLM）  
- Causal Decoder：只能看到前面 token，严格自回归（GPT、LLaMA）  
- Encoder-Decoder：Encoder 全局可见，Decoder 自回归（T5、BART）  
### 大模型LLM的训练目标是什么？
- 自回归语言建模（Causal LM）：最大化当前 token 在已知前序 token 下的概率（GPT）。  
- 自编码语言建模（Masked LM）：随机掩码部分 token，预测其原始值（BERT）。  
- 文本到文本转换（Seq2Seq）：给定输入文本，生成对应输出文本（T5）。  
### 涌现能力是啥原因？
- 数据量增加
- 计算能力提升，gpu出现
- 模型架构改进，比如transformer
### 为何现在的大模型大部分是Decoder only结构？
它更适合生成任务，并且通过自回归生成文本能保持高效性和连贯性。此结构简化了训练和推理过程，可以在大规模数据上进行预训练。
### 简单介绍一下大模型【LLMs】？
通常使用 Transformer 结构
### 大模型【LLMs】后面跟的 175B、60B、540B等指什么？
参数数量
### 大模型【LLMs】具有什么优点和缺点
- 优点：跨任务能力，自适应性
- 缺点：计算资源要求高，缺乏可解释性
## Layer normalization 篇
#### Layer normalization-方法篇
##### Layer Norm 的计算公式

$$
y_i = \gamma \left( \frac{x_i - \mu}{\sqrt{\sigma^2 + \epsilon}} \right) + \beta
$$

##### RMS Norm 的计算公式？RMS Norm 相比于 Layer Norm 有什么特点？
$$
   y_i =  \gamma \left( \frac{x_i }{\sqrt{\sigma^2 + \epsilon}} \right) + \beta
$$
RMS Norm 只使用输入的平方均值进行标准化，而不涉及均值。
##### Deep Norm 思路？Deep Norm 有什么优点？
通过多级归一化和自适应参数，更有效传播梯度，增强训练稳定性
#### Layer normalization-位置篇
##### LN 在 LLMs 中的不同位置 有什么区别么？如果有，能介绍一下区别么？
- 自注意力层之前：归一化输入，帮助稳定注意力权重计算。
- 前馈网络之前：归一化输出，确保输入前馈网络时的稳定性。
#### Layer normalization 对比篇
##### LLMs 各模型分别用了 哪种 Layer normalization？
大多数当前的 LLMs（如 GPT 系列、BERT、RoBERTa、T5、LLaMA 等）都采用了 标准 Layer Normalization（LayerNorm）且大多使用 Pre-LN 结构，即将 Layer Normalization 放在每个 Transformer 子层的输入端。

## LLMs 激活函数篇
### 激活函数的作用
引入非线性
### 介绍一下 FFN 块计算公式？
$$
y = W_2 \, \text{ReLU}(W_1 x + b_1) + b_2
$$
FFN 通常由两个线性层组成，第一层将输入投影到一个更高维度的空间（隐藏层），第二层再将其投影回原始维度。FFN 在每个 Transformer 层中作用于自注意力的输出，是 Transformer 模型进行非线性变换的关键组件

### 介绍一下 Gaussian Error Linear Unit（GeLU） 计算公式？
$$
\text{GeLU}(x) = 0.5 x \left( 1 + \tanh\left( \sqrt{\frac{2}{\pi}} \left( x + 0.044715 x^3 \right) \right) \right)
$$

### 介绍一下 Swish 计算公式？
$$
\text{Swish}(x) = x \cdot \frac{1}{1 + e^{-x}}
$$

### GLU作用
精细地控制信息的流动

### 介绍一下 使用 GLU 线性门控单元的 FFN 块计算公式？
$$
y = \left(W_1 x + b_1 \right) \odot \sigma\left(W_2 x + b_2\right)
$$

### 介绍一下 使用 GeLU 的 GLU 块计算公式？
$$
y = \left(W_1 x + b_1 \right) \odot \text{GeLU}\left(W_2 x + b_2\right)
$$

### 各LLMs都使用哪种激活函数？
GeLU 因其平滑性和高效性被广泛用于大多数现代 LLM（如 GPT、BERT、RoBERTa 等），而 ReLU 由于简单且计算高效，常用于一些其他模型，如 T5。

### Adam优化器和SGD的区别？
Adam优化器通过自适应学习率和梯度动量调整来加速收敛并提高稳定性，而SGD使用固定学习率，收敛较慢但计算开销较小。

## LLMs损失函数篇
### 介绍一下 KL 散度？
衡量两个概率分布之间的差异。
$$
D_{KL}(P \parallel Q) = \sum P(x) \log \frac{P(x)}{Q(x)}
$$

### 交叉熵损失函数写一下，物理意义是什么？
$$
\text{Cross Entropy} = - \sum y_i \log p_i
$$
衡量模型预测和真实分布之间的差距。

### KL 散度与交叉熵的区别？
KL 散度是两个分布之间的非对称差异度量，而交叉熵损失衡量的是预测分布与真实分布之间的差距，交叉熵包含了KL散度和真实分布的熵。

### 多任务学习各loss差异过大怎样处理？
为不同任务的损失函数加权

### 分类问题为什么用交叉熵损失函数不用均方误差（MSE）？
交叉熵更适合概率分布间的比较，MSE用于回归问题，不能有效处理概率分布的差异。

### 什么是信息增益？
衡量一个特征对数据集分类信息的贡献，通常用于决策树算法。

### 多分类的分类损失函数
多分类问题通常使用Softmax 激活函数结合交叉熵损失函数，Softmax将输出转化为概率分布，交叉熵计算真实标签和预测标签的差异。

### softmax和交叉熵损失怎么计算，二值交叉熵呢？
- softmax：
$$
P(y_i) = \frac{e^{z_i}}{\sum_j e^{z_j}}
$$
- 交叉熵损失：
$$
\text{Cross Entropy} = - \sum y_i \log p_i
$$
- 二值交叉熵：
$$
H(y, p) = - [y \log p + (1 - y) \log (1 - p)]
$$

### 如果softmax的e次方超过float的值了怎么办？
为了防止溢出，通常通过减去最大值来稳定计算：
$$
\text{Softmax}(z_i) = \frac{e^{z_i - \max(z)}}{\sum_j e^{z_j - \max(z)}}
$$

# 大模型进阶面

### 什么是 LLMs 复读机问题？为什么会出现 LLMs 复读机问题？如何缓解 LLMs 复读机问题？

### llama 输入句子长度理论上可以无限长吗？

### 如何让大模型处理更长的文本？


# 微调面
## 微调面

### sft和peft区别
SFT 是用带标签的训练数据来训练所有参数来适应特定任务。PEFT只微调部分参数，保持大部分预训练模型不变。

### 大模型 sft 过程中，为什么会出现第二个epoch的时候loss会突然下降问题？
因为是大模型，参数量很大，在第一个epoch结束的时候就记住了整个训练集，所以第二个epoch会突然下降。

### 如果想要在某个模型基础上做全参数微调，需要多少显存？
目前大部分 LLM 默认为半精度，当在模型大小为 XB ，推理所需显存约为 X 的两倍。全参数微调所需显存是 X 的 6-8 倍。但实际测试看来全参所需显存约为推理所需显存的 8-10 倍。

### 为什么 SFT 之后感觉 LLM 傻了?
指令微调是为了增强（或解锁）大语言模型的能力。对于数据集应该选择多个有代表性的任务，每个任务实例数量不应太多，否则可能会潜在地导致过拟合问题并影响模型性能 。
同时，应该平衡不同任务的比例，并且限制整个数据集的容量（通常几千或几万），防止较大的数据集压倒整个分布。

### 领域模型 Continue PreTrain 数据选取？
书籍、论文

### 领域数据训练后，通用能力往往会有所下降，如何缓解模型遗忘通用能力？
在领域训练的过程中加入通用数据集。一般领域数据与通用数据的比例在 1:5 到 1:10 之间。

### 领域模型Continue PreTrain ，如何让模型在预训练过程中学习到更多的知识？
添加下游 SFT 的数据

### 进行SFT操作的时候，基座模型选用Chat还是Base?
在进行 SFT 时，选用 Chat 还是 Base 作为基座，需要根据 SFT 的数据量决定。如果数据量小于 10k，建议选用 Chat 模型作为基座进行微调；如果有 100k 以上的数据，建议在 Base 模型上进行微调。

###  指令微调数据有什么格式要求？
指令微调也是一种监督微调（sft）。指令数据一般为 json 格式，包含 Instruction、Input、Output 三个字段（Instruction 或 Input 可以为空）。
Instruction（指令）：Instruction 是对模型的输入文本或问题进行进一步说明或约束的指令。它可以是一种特定的格式或标记，用于告诉模型如何处理输入数据，或者提供一些额外的信息，以便模型更好地理解和生成输出。Instruction的作用是为了描述任务。
Input（输入）：Input 是模型实际接收的文本或数据。提供了指令待处理的对象，用于生成输出。
Response（输出）：Response 是 LLM 生成的输出文本或回答。它是模型对 Instruction 和 Input 的处理结果。 Response 的内容取应当是理想的文本、回答、建议、解释等形式。

### 提升 SFT 的 Prompt 代表性的方法
- 覆盖不同任务类型
- 多样化指令表达
- 加入挑战性数据

### 提升 SFT 的 Prompt 数据量的方法
- 模型生成数据
- 数据增强
- 爬取开源数据

### 领域模型评测集如何构建？
- 构建专业知识选择题数据集
- 构建主观题数据集

### 领域模型词表扩增是不是有必要的？
不一定需要扩充领域模型词表，词表扩增真实解决的问题是解码效率的问题，给模型效果带来的提升可能不会有很大。但有些领域比较特殊，需要扩充词表。比如某些领域存在特定词汇，对于目标领域内存在的晦涩难懂的专有术语或特殊词汇，若这些词汇并不存在词表中，扩展词表就是有必要的。将这类特殊术语纳入模型的词库，能够增强模型对这些术语的理解与处理能力。

### 训练中文大模型有什么经验？
如果模型在预训练阶段就包含足够的中文如 ChatGLM、Baichuan，可以直接利用其进行微调，如果模型在预训练阶段无中文或者中文数据含量较少的模型如 LLaMA，需要对其 tokenizer 进行修改，添加中文字词，同时进行二次预训练使其充分理解中文再进行 SFT 微调。

### 对 LLM 进行指令微调的好处？
- 行为引导：LLM 的行为往往不易解释和控制。借助指令微调，我们可以输入指令使其学习，以引导模型行为，使其更好地满足特定任务的要求。
- 数据效率：LLM 训练通常依赖于大量数据，但在一些特定的任务或领域里，关键数据往往不够充足或难以采集。指令微调可以使我们仅利用少量特定任务的数据，结合大型模型的通用数据预训练知识，在数据受限的条件下也能够取得优异的性能。

### 预训练阶段注入知识还是微调阶段注入知识？
知识是在预训练阶段注入的，而微调阶段是指在特定任务上的训练，以使预训练模型的通用知识和特定任务的要求相结合，激发模型的能力，使模型在特定任务上表现更好。

### 想让模型学习某个领域或行业的知识，是应该预训练还是应该微调？
对于大模型来说是在预训练的阶段注入领域知识的，因此在训练行业大模型的时候最佳的方案是使用预训练与微调相结合的方案，先用篇章数据进行预训练以获取广泛的知识，再用问答数据进行微调，使模型更好的学习到特定领域的知识。

### 多轮对话任务如何微调模型？

### 为什么微调后的模型会出现灾难性遗忘？
灾难性遗忘是指在模型微调过程中，当模型在新任务上进行训练时，可能会忘记之前学习到的知识，导致在旧任务上的性能下降。微调过程中使用的新任务数据与预训练数据或旧任务数据的分布存在差异。如果新任务的数据分布与预训练数据差异较大，模型可能会过度调整以适应新任务，导致旧任务上的性能下降。同时随着 epoch 的上升，对旧任务的遗忘也会越多，通常 epoch 只用设置为 1（具体需要看数据量大小和任务需求）。

### LLM 进行 SFT 操作的时候在学习什么？
在有监督的数据集上进行 SFT 训练，可以利用上下文信息等监督信号进一步优化模型。当进行有监督微调时，模型权重会根据与真实标签的差异进行调整。通过这个微调过程，模型能够捕捉到标签数据中特定于某些任务的模式和特点。使得模型更加精确，更好地适应某一特定任务。

### 预训练和 SFT 操作有什么不同
预训练是通过无监督学习从大规模的文本语料库中学习语言模型的表示能力和语言知识，让模型学会接龙。

SFT 的目标是在特定的任务上进行训练，模型会利用预训练阶段学到的语言表示和知识，通过有监督的方式调整模型参数，以适应特定任务的要求。

### 样本量规模增大，训练出现 OOM 错误如何解决？
- 减小批量大小
- 使用梯度累计
- 使用模型并行

## 大模型SFT Trick篇
### 常见SFT的开发流程是如何的？
- 根据业务场景调整提示词
- 尝试闭源和开源，并进行对比
- 认真准备数据集
- 上线迭代
- 进行训练、上线和持续的迭代优化

### 训练数据要注重什么？
- 确保回答格式和风格的统一
- 数据集既要包含难也要包含易
- 避免引入模型在预训练阶段未接触过的知识

### 大 size 和小 size 模型的选择？
在效率和资源都达标和到位的情况上，优先用大 size 的模型进行实验和微调，因为大 size 的模型在容错性上比小 size 的好太多。尽管大尺寸模型也可能存在多任务不稳定、标签不平衡等问题，但其表现通常会比小尺寸模型更为稳定。因此，选用大尺寸模型其实是节省了人力成本，避免了很多之后可能会遇到的各种坑。

### 多任务训练时怎么确保每个任务都优秀？
- 不同任务独立训练模型
- 任务取舍与额外训练

### SFT真的不能学到知识？
常识和世界知识难以通过 SFT 灌输给模型。应该关注SFT在以下方面：
- 激发预训练知识
- 稳定格式输出
- 更遵循具体任务
- 学习业务逻辑

### 怎么科学挑选数据集？
- 精简业务数据
- 筛选开源数据
- 诡探数据秘密

### 怎么解决幻觉问题
尚缺乏完美的解决方案，但可以考虑通过SFT或当积累更多相关场景数据后尝试运用强化学习方法来改善。

### 该选什么微调方法， Full tuning、P-tuning、Lora?
当数据量仅在几千条时，P-tuning是最佳选择；数据量在几千至万条之间时，Lora更为适合；而当数据量达到百万级别时，Full-tunning效果最佳。此外，使用 Full-tunning 会导致训练后的模型泛化性不如 Lora。在大多数场景下，我仍推荐使用Lora，因为稳定，效果不差，能尽可能保留模型原来的泛化性。

### SFT 还有什么方面值得研究？
- 消除幻觉
- 精选数据集和任务配比
- 设置科学设计问答格式和 Data format
- 探寻更高效的微调技巧

## 大模型训练经验
### 分布式训练框架选择？
- DeepSpeed：由微软开发，优化显存使用，支持大规模训练。
- Megatron-LM：NVIDIA 开发，专为训练大规模 Transformer 模型（如 GPT）设计。

###  模型训练样本量规模增大，导致训练任务直接报OOM了，该怎么办？
使数据向量化耗时随处理进程的增加线性下降

# 大模型（LLMs）langchain面
## 大模型（LLMs）langchain 面
### 什么是 LangChain?
LangChain 是一个专为大语言模型（LLM）设计的 开源框架，用于构建、管理和优化LLM 应用

### LangChain 包含哪些核心概念？
- Components and Chains：Component 是模块化的构建块，可以组合起来创建强大的应用程序。Chain 是组合在一起以完成特定任务的一系列 Components（或其他 Chain）。
- Prompt Templates and Values：Prompt Template 负责创建 PromptValue，这是最终传递给语言模型的内容。Prompt Template 有助于将用户输入和其他动态信息转换为适合语言模型的格式。PromptValues 是具有方法的类，这些方法可以转换为每个模型类型期望的确切输入类型（如文本或聊天消息）。
- Example Selectors：当您想要在 Prompts 中动态包含示例时，Example Selectors 很有用。他们接受用户输入并返回一个示例列表以在提示中使用，使其更强大和特定于上下文。
- Output Parsers：Output Parsers 负责将语言模型响应构建为更有用的格式。它们实现了两种主要方法：一种用于提供格式化指令，另一种用于将语言模型的响应解析为结构化格式。这使得在您的应用程序中处理输出数据变得更加容易。
- Indexes and Retrievers：Index 是一种组织文档的方式，使语言模型更容易与它们交互。检索器是用于获取相关文档并将它们与语言模型组合的接口。LangChain 提供了用于处理不同类型的索引和检索器的工具和功能，例如矢量数据库和文本拆分器。
- Chat Message History：LangChain 主要通过聊天界面与语言模型进行交互。ChatMessageHistory 类负责记住所有以前的聊天交互数据，然后可以将这些交互数据传递回模型、汇总或以其他方式组合。这有助于维护上下文并提高模型对对话的理解。
- Agents and Toolkits：Agent 是在 LangChain 中推动决策制定的实体。他们可以访问一套工具，并可以根据用户输入决定调用哪个工具。Tookits 是一组工具，当它们一起使用时，可以完成特定的任务。代理执行器负责使用适当的工具运行代理。

# 大模型（LLMs）RAG 检索增强生成面
## 大模型（LLMs）RAG 入门篇
### 基于LLM+向量库的文档对话 基础面
#### 为什么大模型需要外挂(向量)知识库？
- 知识过时
- 上下文窗口限制
- 精确知识检索
- 降低计算成本

#### 基于LLM+向量库的文档对话思路是怎么样？
- 数据预处理：分词、去除停用词、词干化等
- 文档向量化：使用向量库的方法，将每个文档表示为一个向量
- 大语言模型训练
- 文档检索：当用户提供一个查询文本时，首先对查询文本进行向量化，然后计算查询向量与文档向量之间的相似度。
- 文档推荐
- 对话交互

####  基于LLM+向量库的文档对话核心技术是什么？
- 大语言模型
- 文档向量化
- 相似度计算
- 对话生成
- 对话交互

#### 基于LLM+向量库的文档对话 prompt 模板如何构建？
- 查询内容
- 查询类型
- 上下文信息
- 可变参数

### RAG面
#### LLM不足
- 幻觉问题
- 时效性
- 数据安全

#### 什么是RAG
即 LLM 在回答问题或生成文本时，先会从大量文档中检索出相关的信息，然后基于这些信息生成回答或文本，从而提高预测质量。

#### 检索器模块中，如何获得准确的语义表示
- 块优化
- 微调嵌入模型

#### 检索器模块中，如何协调查询和文档的语义空间？
- 查询重写：伪文档
- 嵌入变换：在查询编码器后加一个特殊的适配器并对其微调

#### 检索器模块中，如何对齐检索模型的输出和大语言模型的偏好？
- 大语言模型的监督训练：REPLUG使用检索模型和大语言模型计算检索到的文档的概率分布，然后通过计算 KL 散度进行监督训练。
- 在检索模型上外部附加适配器来实现对齐
- PKG通过指令微调将知识注入到白盒模型中，并直接替换检索模块，用于根据查询直接输出相关文档

#### 生成器的作用
将检索到的信息转化为自然流畅的文本

#### 生成器模块中，如何通过后检索处理提升检索结果？
后检索处理指的是，在通过检索器从大型文档数据库中检索到相关信息后，对这些信息进行进一步的处理、过滤或优化。

#### 如何优化生成器应对输入数据？
其优化目的：在于确保生成文本既流畅又能有效利用检索文档，更好地回应用户的查询。可以对生成器微调，具体方法和大语言模型的普通微调方法大体相同。

#### 使用rag的好处
- 可扩展性
- 准确性
- 可控性
- 可解释性
- 多功能性
- 及时性
- 定制性

#### RAG 典型实现方法
- 数据索引：数据提取，文本分割，向量化，创建索引
- 检索
- 生成

# 大模型（LLMs）参数高效微调(PEFT)面
## 大模型（LLMs）参数高效微调(PEFT)面
### 微调方法是啥？如何微调？
在一个已经预训练好的模型基础上，通过进一步训练来适应特定的任务或数据集

### 为什么需要 PEFT？
节省计算资源和时间

### 介绍一下 PEFT？

## 适配器微调
### 为什么需要适配器微调（Adapter-tuning）？
- 避免灾难性遗忘：在全参微调方法中，微调过程可能会导致预训练模型在原任务上的性能下降，即灾难性遗忘。
- 减少微调的计算量和时间
- 提高模型的可解释性和可复用性

###  适配器微调（Adapter-tuning）思路？
在预训练模型每一层(或某些层)中添加 Adapter 模块(如下图左侧结构所示)，微调时冻结预训练模型主体，由Adapter模块学习特定下游任务的知识。每个 Adapter 模块由两个前馈子层组成，第一个前馈子层将 Transformer 块的输出作为输入，将原始输入维度 d 投影到 m，通过控制m的大小来限制 Adapter 模块的参数量，通常情况下 m << d。在输出阶段，通过第二个前馈子层还原输入维度，将m重新投影到 d，作为 Adapter 模块的输出(如下图右侧结构)。

### 适配器微调（Adapter-tuning）特点是什么？
- 保留预训练模型的知识
- 提高模型的可解释性和可复用性
- 减少微调的计算量和时间
- 灵活性和可扩展性

### AdapterFusion 思路是什么？
首先针对于每个任务，学习一组新的 adapter 参数。然后，针对于某个特定目标任务，学习一个融合模块把第一步的所有 adapter 结合起来。

### AdapterDrop 思路是什么？
为了加快推理速度，在推理时可以对某几层的 adapter 进行剪枝。根据的结论，靠近输入的 adapter 被剪掉后对性能影响更小。

### AdapterDrop 特点是什么？
- 动态适配器选择
- 鲁棒性和泛化能力
- 减少计算量和参数数量

## 提示学习（Prompting）
### 为什么需要提示学习（Prompting）？
- 解决模糊性
- 控制生成
- 纠正偏见
- 增加一致性

### 什么是提示学习？
提示学习（Prompting）是一种在机器学习中使用人类编写的提示或示例来辅助模型进行学习和推理的技术。

### 提示学习有什么优点？
- 控制生成输出
- 提高生成质量
- 解决数据稀疏问题
- 提供可解释性

### 提示学习（Prompting）有哪些方法，能不能稍微介绍一下它们？
- 文本前缀
- 问题模板
- 知识引导

### 为什么要前缀微调
传统的预训练语言模型在生成任务中存在一些问题：
- 缺乏控制
- 缺乏指导
- 数据偏差
前缀微调通过在输入文本的开头添加一个人工设计的前缀，将任务要求或指导信息引入到生成过程中，从而解决了上述问题。

### 前缀微调（Prefix-tining）思路是什么？
在预训练语言模型的基础上，通过微调的方式引入任务相关的指导信息，从而提高模型在特定生成任务上的性能和可控性。

### 前缀微调（Prefix-tining）的优点是什么？
- 可控性
- 数据效率：可以在相对较少的任务数据上进行微调
- 可解释性

### 前缀微调（Prefix-tining）的缺点是什么？
- 前缀设计的挑战
- 任务依赖性

###  为什么需要指示微调（Prompt-tuning）？
指示微调（Prompt-tuning）是一种用于生成任务的微调方法，它的出现主要是为了解决前缀微调（Prefix-tuning）中前缀设计的挑战和限制。
- 前缀设计的复杂性
- 指导信息的一致性
- 任务的多样性和灵活性
- 模型的可解释性

## Lora系列篇
### 什么是Lora？
是一种针对大型语言模型进行低秩适应的技术。低秩适应的目标是通过将语言模型的参数矩阵分解为低秩近似，来减少模型的复杂度和计算资源的需求。

### Lora的思路是什么
LoRA 的核心思想是对大型模型的权重矩阵进行隐式的低秩转换，整体的想法和概念与主成分分析（PCA）和奇异值分解（SVD）有关。通过对模型参数矩阵进行低秩分解，将其分解为两个或多个较小的矩阵的乘积作为旁路分支。然后仅调整低秩的近似矩阵，而冻结原模型的参数。这样可以减少模型的参数量和计算复杂度，同时保留模型的关键特征和性能。

### Lora的特点是什么
- 低秩适应
- 保持关键特征
- 减少存储需求
- 加速推理过程
- 可扩展性

### QLora的思路是怎么样的
解决当前微调特别大的模型成本太高
- 4bit量化
- 二次量化
- paged optimizers：可以在 GPU RAM 偶尔不足的情况下实现 CPU 和 GPU 之间的数据传输，确保训练不会中断。

### AdaLoRA 的思路是怎么样的？
- SVD形式参数更新
- 基于重要程度的参数分配：裁剪一些冗余的奇异值

### LoRA微调方法为什么能加速训练？
- 冻结了大量的参数
- 降低了计算复杂度
- 加速收敛速度

### 如何在已有 LoRA 模型上继续训练？
可先将模型与 LoRA 训练出来的权重进行合并，使其重新组合成原始模型的参数矩阵，然后就可以再次进行 LoRA 微调了。

### LoRA 这种微调方法和全参数比起来有什么劣势？
牺牲一部分模型质量，但将获得更高效地提供多个模型服务的能力。同时，LoRA 在特定的应用领域中表现比较出色，但可能在需要逻辑推理等更广泛的任务中表现欠佳。

### 为什么矩阵B被初始化为0，而矩阵A正常高斯初始化
在训练开始时维持网络的原有输出(初始偏移为0)，但同时也保证在真正开始学习后能够更好的收敛
- 如果B，A全都初始化为0，那么缺点与深度网络全0初始化一样，很容易导致梯度消失(因为此时初始所有神经元的功能都是等价的)。
- 如果B，A全部高斯初始化，那么在网络训练刚开始就会有概率为得到一个过大的偏移值 Δ W 从而引入太多噪声，导致难以收敛。

### LoRA与Adapter的区别
- 插入位置。LoRA是以残差连接的形式"并联"在Transformer的Q,K,V,O矩阵上，而Adapter是插入在Feed-forward Layer后面。
- 推理延迟。LoRA在训练完后其参数可以与原有预训练模型直接合并，变回单分支结构，不会引入额外的延迟；而Adapter由于引入了额外的串联网络层，因此会带来额外的延迟。
- 参数存储。使用LoRA进行微调，在训练完毕后只需要保存LoRA本身的参数；而使用Adapter则要保存整个原有模型的参数。

# 大模型强化学习面
## 大模型强化学习面
### 简单介绍强化学习
一种机器学习技术，可以训练模型做出决策，以实现最佳结果。它模仿了人类为实现目标所采取的反复试验的学习过程。有助于实现目标的模型决策会得到加强，而偏离目标的操作将被忽略。

### 简单介绍RLHF
- 预训练语言模型
- 训练奖励模型
- 用强化学习微调

### RLHF 在实践过程中存在哪些不足？
- 人类反馈的代价高昂
- 人类反馈的主观性
- 错误反馈的影响
- 缺乏探索与利用的平衡：人类反馈通常用于指导模型的行为，但可能会导致模型过于依赖人类反馈而缺乏探索的能力

### 如何解决人工产生的偏好数据集成本较高，很难量产问题？
- 引入模拟数据
- 利用已有的高质量数据
- 众包和协作
- 数据增强和迁移学习

### 如何解决三个阶段的训练（SFT->RM->PPO）过程较长，更新迭代较慢问题？
DPO 用 二元交叉熵损失 直接优化 LLM，使其更符合人类偏好，避免了 RLHF 复杂的奖励建模和强化学习过程。这样既降低了训练成本，又提高了对齐效率，是一种更简单高效的偏好优化方法。

###  如何解决 PPO 的训练过程同时存在4个模型（2训练，2推理），对计算资源的要求较高问题？
RRHF只需要1-2个模型

### 有监督微调（Supervised Tinetuning）的训练数据格式是什么样？
- aplaca
- sharegpt

### 简单介绍一下 对齐（Alignment）？
对齐（Alignment）指的是让大语言模型（LLM）的行为更符合人类期望，包括生成更有用、安全、公平的回答，以减少偏见、幻觉或误导性内容。

## 大模型强化学习-PPO面
### 大语言模型RLHF中的PPO主要分哪些步骤？
- 策略生成
- 奖励评估
- 策略更新

### PPO的采样过程
模型从策略分布中采样多个回复，计算奖励并优化策略。

### PPO的采样策略
常见策略包括贪心搜索（Greedy）、Top-k 采样、温度调节（Temperature Scaling）等。

### PPO的收益评估
收益通常由 Reward Model 评估，可以基于人类偏好或特定指标计算得分，如可读性、一致性、相关性等。

## RLHF平替算法DPO篇
### 介绍一下 DPO的损失函数？
$$
\mathcal{L}_{\text{DPO}} = -\mathbb{E}_{(x, y_w, y_l) \sim D} \left[ \log \sigma \left( r_{\theta}(x, y_w) - r_{\theta}(x, y_l) \right) \right]
$$  

### DPO如何简化RLHF
- 去掉强化学习
- 不需要采样新的回复：使用固定数据集
- 稳定性更强：避免RLHF中梯度爆炸等问题

### DPO 第 0 步 Loss 是否固定？如果固定，值是多少？
是固定的， 0.693，因为未训练时胜负回答得分差值为0。

### DPO 是 on-policy 还是 off-policy 算法？
off-policy，因为它直接基于固定数据集训练，而不需要在线采样新的数据。

## reward篇
### 为什么不直接打分，而是标排列序列？
一致性更强：不同人打分的标准可能不同，但相对排序（A 比 B 好）更稳定。

### Reward Model 的 Loss 是怎么计算的？
Pairwise Ranking Loss（对比损失）：
$$
L = -\log \sigma (r_A - r_B) = -\log \frac{1}{1 + e^{-(r_A - r_B)}}
$$

# 大模型训练集面
## 大模型训练集面
### SFT（有监督微调）的数据集格式？
指令-响应
### RM（奖励模型）的数据格式？
排序对比 
### PPO的数据格式
模型生成的多个候选答案，并结合 RM 评分

# 大模型（LLMs）分布式训练面
## 大模型（LLMs）分布式训练面
### 什么是点对点通信
两个计算单元之间直接进行数据交换的通信方式。
### 什么是集体通信
在分布式系统中，多个计算单元之间的通信协调。
### 什么是数据并行
在深度学习中，数据并行通常是指将数据集拆分成多个小批次（mini-batch），然后在多个设备上并行地训练模型。每个设备计算自己的梯度，然后通过集体通信将这些梯度进行合并（如求和），从而更新全局模型。
### 什么是流水线并行
流水线并行是一种通过将计算任务分解为多个阶段并将数据流经过这些阶段的并行计算方法。在深度学习中，流水线并行将模型的不同层分配到不同的处理单元，每个处理单元负责计算模型的一部分。这种方法允许在不同的计算节点之间同时执行不同的计算任务，从而加速整体计算过程。
### 什么是张量并行
张量并行是指在处理大型张量（如神经网络中的权重矩阵或激活矩阵）时，将张量拆分成多个部分并在不同的计算单元上并行处理的技术。
### 什么是3D并行
结合了 数据并行、张量并行 和 流水线并行 三种并行方式的优势。在多个维度上将模型切分和分配计算负载，最大程度地提高资源利用率，并减少训练时间。
### 如果有 N 张显存足够大的显卡，怎么加速训练？
- 数据并行：将训练数据拆分成多个小批次，分配到不同的显卡上进行并行计算，最后汇总结果更新模型。
- 模型并行：将模型的不同部分（如不同层）分配到不同显卡，显卡之间协作计算。
### 如果显卡的显存不够装下一个完整的模型呢？
- 模型并行
- 混合精度训练
- 梯度累积：将多个小批次的梯度累积后再更新参数，减少显存占用。
### PP 推理时，是一个串行的过程，1个 GPU 计算，其他空闲，有没有其他方式？
- 流水线并行：将推理任务分解成多个阶段，每个阶段在不同的计算单元上执行，减少单个显卡的等待时间。
- 数据并行：将推理过程分配到多个 GPU 上，同时计算不同输入数据的推理结果。
### 除了3D并行有没有其他方式大规模训练？
ZeRO（Zero Redundancy Optimizer）：通过分布式梯度下降的优化，减少冗余的存储和计算，提升大规模训练效率。
### 有了ZeRO系列，为什么还需要3D并行？
虽然 ZeRO 系列优化了内存使用，但它主要聚焦于数据和梯度的分布式优化。3D 并行则是从计算并行的角度出发，通过数据并行、张量并行和流水线并行的结合，进一步提升计算效率。两者可以互补使用，ZeRO 处理内存优化，3D 并行提升计算性能。
- ZeRO-1：仅共享梯度。每个设备只存储自己的梯度和模型参数的子集。
- ZeRO-2：共享梯度和优化器状态。每个设备只存储自己的一部分参数、梯度和优化器状态。
- ZeRO-3：在ZeRO-2的基础上进一步将模型参数本身也进行切分，每个设备只存储一部分模型参数、梯度和优化器状态。
### 平民适不适合玩 3D 并行？
3D 并行通常涉及大量的计算资源和显卡，需要高效的硬件支持。对于普通用户或较低预算的用户，3D 并行可能不太适合，因为这需要多张显卡、强大的网络连接和充足的内存带宽。
### 显存优化技术有哪一些，都有什么特点？
- 混合精度训练：使用较低的精度（如16位浮点数）来减小内存占用。
- 梯度累积：将多个小批次的梯度累积后再进行参数更新，减少显存占用。
- 模型并行：将模型的不同部分分配到不同显卡，减轻单卡显存的压力。
### 常见的分布式训练框架哪一些，都有什么特点？
- Horovod：基于 MPI 的框架，广泛用于大规模分布式训练。
- DeepSpeed：由 Microsoft 开发，提供了优化的内存管理、分布式训练和超大模型的支持。

## 流水线并行
### 流水线并行的优化目标是什么
- 训练更大的模型
- 更快的训练模型

### 流水线并行的分类
- 朴素流水线并行
- 微批次流水线并行
    - GPipe：F-then-B
    - PipeDream：1F1B(前向计算和反向计算交叉,解决内存过高的问题)：Deepseed

## 数据并行
### 为什么需要数据并行
由于训练数据集太大；因此，将数据集分为N份，每一份分别装载到N个GPU节点中，同时，每个GPU节点持有一个完整的模型副本

### 数据并行的缺点：
- 单进程多线程带来的问题
- 效率问题，主卡性能和通信开销容易成为瓶颈，gpu利用率低
- 不支持模型并行

### 介绍一下 nn.DataParallel 函数？
单进程多线程进行实现的，它使用一个进程来计算模型权重，在每个批处理期间将数据分发到每个GPU。

### nn.DataParallel 函数 处理逻辑 介绍一下？
- 模型复制
- 数据划分
- 并行计算：每个gpu独立计算前向传播和反向传播
- 梯度汇总：计算出的梯度汇总到主gpu，主gpu负责梯度聚合和更新模型参数

## 分布式数据并行
### 为什么需要 nn.parallel.DistributedDataParallel？
DP存在问题：
- 主gpu计算瓶颈
- 数据传输开销大

### DP和DDP区别
- DP 是基于单进程多线程的实现，只用于单机情况，而 DDP 是多进程实现的，每个 GPU 对应一个进程，适用于单机和多机情况，真正实现分布式训练
- DDP在各进程梯度计算完成之后，各进程需要将梯度进行汇总平均，然后再由 rank=0 的进程，将其广播到所有进程后，各进程用该梯度来独立的更新参数（而 DP是梯度汇总到 GPU0，反向传播更新参数，再广播参数给其他剩余的 GPU）。
- DDP 支持模型并行，而 DP 并不支持，这意味如果模型太大单卡显存不足时，只能使用DDP。

### 解释Ring-AllReduce？
使用环形结构进行梯度聚合，每个gpu只与相邻gpu进行通信，使得通信开销最小化。只需要发送和接受2（N-1）次数据，更节省带宽。

### 适用于DDP的显存优化技术
由于 DDP 会在每个 GPU 上存储一份完整的模型，它的显存消耗较大，可以结合 ZeRO (Zero Redundancy Optimizer) 来优化显存：

    ZeRO Stage 1：优化 优化器状态。

    ZeRO Stage 2：优化 梯度存储。

    ZeRO Stage 3：优化 模型参数存储。

## AMP混合精度训练
### 为什么需要AMP混合精度训练？
自动混合精度可以在训练过程中针对不同的层，采用不同的数据精度进行计算，从而实现节省显存和加快速度的目的。

## pytorch的deepseek详细解析
### 为什么需要deepseed
- 主要解决大规模模型训练和推理的计算，显存和通信问题
- ZeRO技术让deepseed比传统DDP更节省内存，在更少gpu上完成更大模型训练
- 3D并行让deepseed适用于超大规模模型训练

### deepseed基本概念
- 节点编号：分配给系统中每个节点的唯一标识符，用于区分不同计算机之间的通信
- 全局进程编号：分配给整个系统中的每个进程的唯一标识符
- 局部进程编号：分配给单个节点内的每个进程的唯一标识符
- 全局总进程数
- 主节点：需要知道ip和端口号

### deepseed的通信策略
- mpi：cpu集群上的分布式训练
- gloo：cpu和gpu
- nccl：gpu专用通信库

## megatron-lm篇
### Activation Recomputation 是怎么实现的？
节省显存。标准前向传播过程中，计算得到的中间激活值通常会被存储，以便在反向传播时使用。激活重计算 通过 不存储 这些中间激活值，而是在反向传播时 重新计算 需要的激活值，从而减少显存占用。

### Megatron 中 OverlappedDistributed Optimizer 是如何实现的？
- 参数分片（ZeRO）
- 梯度计算与参数更新并行化
- 使用异步通信：CUDA Streams

### Megatron-LM 中 Context Parallel 介绍
不同 GPU 处理不同的 Token 或上下文（Context），减少单个 GPU 计算负担。

### Context Parallel vs 数据并行
- 数据并行： 多个 GPU 处理不同数据，最后同步梯度。
- Context 并行： 每个 GPU 只处理部分上下文，不同步梯度，节省显存。

# LLMs位置编码篇

## 为什么需要位置编码
由于 Transformer 自身具有置换不变性，无法直接捕获每个词在序列中的位置信息，因此要使用位置编码将序列中元素顺序信息融入Transformer。

## 什么是绝对位置编码？
为每个输入的词汇分配一个固定的、与其在序列中所处位置相关的编码
### 训练式绝对位置编码
将位置编码当作可训练参数。没有外推性。再模型的嵌入层引入可学习的参数
### 三角函数式绝对位置编码
有外推性
## 什么是相对位置编码
关注的是词语之间的相对位置。相对位置编码使得模型不仅可以在短序列中捕捉到位置信息，还能扩展到处理长序列时，避免过度依赖绝对位置信息。通过修改自注意力计算的过程，把相对位置信息植入到Transformer架构的每一层的注意力机制中。
## 旋转位置编码RoPE的思路
用于gpt4。首先在q和k向量中引入绝对位置信息，在attention运算中进行内积，此时融入相对位置。
## 长度外推问题
### 什么是长度外推问题
大模型的外推性问题是指大模型在训练时和预测时的输入长度不一致，导致模型的泛化能力下降的问题。在目前的大模型中，一般指的是超出预训练设置的上下文长度时，依旧保持良好推理效果的能力。
### 长度外推问题的解决方法有哪些
- 进制表示
- 直接外推
- 线性插值
- 进制转换
### 为了做到长度外推性，需要解决两个主要问题
- 预测时位置编码的外推
- 预测时序列更长，导致注意力相比训练时更分散
## ALiBi (Attention with Linear Biases) 思路是什么？
不直接输入position Embedding，然后 QK^T 计算时加入一个偏置，偏置其实就包含了Q和K的元素相对位置.

# LLMs Tokenizer篇
## LLMs Tokenizer篇
### Byte-Pair Encoding(BPE)篇 
#### 介绍一下BPE？
Byte-Pair Encoding (BPE) 是一种常用于自然语言处理（NLP）中的分词算法，旨在将文本数据分割成子词（subwords）单元，特别适用于处理词汇表的稀疏性问题。在BPE中，算法通过反复合并频繁出现的字节对（即连续的两个字符或子字符串），从而逐步构建出一个优化的词汇表。
#### Byte-Pair Encoding(BPE) 如何构建词典？
- 初始化
- 迭代合并
- 完成词典
#### Byte-Pair Encoding (BPE) 优点
- 减少OOV问题
- 词典大小可控
- 兼顾词汇粒度
- 适用于多语言任务
#### Byte-Pair Encoding (BPE) 缺点
- 不考虑语义信息
- 分割可能不自然
- 合并规则固定

### byte-level BPE篇
#### 介绍一下 Byte-level BPE ？
Byte-level BPE（字节级 BPE）是 BPE 的一种变体，直接在 字节（byte） 层面进行合并，而不是基于 Unicode 字符。它在 GPT-2 等模型中使用，以确保对所有语言、符号和特殊字符的兼容性。
#### Byte-level BPE 如何构建词典？
- 初始化词典：将输入文本按字节划分
- 计算频次
- 合并最频繁的字节对
- 重复直至收敛
#### byte-level BPE的优点
- 完全字符集支持
- 无oov问题
- 更灵活的字词合并
#### byte-level BPE的缺点
- 字词序列更长
- 难以利用语言特性

### WordPiece篇
在合并时基于最大似然估计

### 各种模型都使用怎么样的tokenizer？
- BERT：WordPiece
- GPT-2 / GPT-3：Byte-level BPE
- T5：SentencePiece

### 特殊token
- CLS: 表示整个句子（用于分类任务）
- SEP: 句子分隔符（用于句子对任务）
- PAD: 填充短句（保证批量训练时句子等长）
- MASK: 预训练时遮挡单词（让模型预测）
- UNK: 未知单词（词汇表中没有的词）

## 怎么让英文大模型支持中文

### 为什么需要构建中文tokenization？
原始的llama模型对中文的支持不太友好，接下来本文将讲解如何去扩充vocab里面的词以对中文进行token化。

### 如何对原始数据预处理？
每一行为一句或多句话。保存为语料corpus。

### 如何构建中文词库
一般的，目前比较主流的是使用sentencepiece训练中文词库。
运行后会得到tokenizer.model和tokenizer.vocab两个文件。

### 如何使用transformers库加载sentencepiece模型？
它可以用transformers库里面的tokenizer对象加载读取。

### 如何合并英文词表和中文词表？
将原始词表中没有的新加入进去vocab.model。

## 继续预训练篇
### 为什么需要进行继续预训练？
我们新增加了一些中文词汇到词表中，这些词汇是没有得到训练的，因此在进行指令微调之前我们要进行预训练。预训练的方式一般都是相同的，简单来说，就是根据上一个字预测下一个字是什么。
### 如何对继续预训练数据预处理？
先使用tokenizer()得到相关的输入，需要注意的是可能会在文本前后添加特殊的标记，比如bos_token_id和eos_token_id，针对于不同模型的tokneizer可能会不太一样。这里在input_ids前后添加了21134和21133两个标记。
然后将所有文本的input_ids、attention_mask, token_type_ids各自拼接起来（展开后拼接，不是二维数组之间的拼接），再设定一个最大长度block_size，这样得到最终的输入。
### 如何构建模型
我们可以使用同样的英文原模型，但是tokenizer换成我们新的tokenizer.由于tokenizer词表个数发生了变化，我们需要将模型的嵌入层和lm_head层的词表数目进行重新设置：
model_vocab_size = model.get_output_embeddings().weight.size(0)
model.resize_token_embeddings(len(tokenizer))


# 大模型幻觉面
### 什么是大模型幻觉？
大模型幻觉（AI Hallucination） 指的是 大模型生成错误、虚假的内容，这些内容看起来合理但实际不真实。
### 为什么会产生幻觉
### 为什么需要解决幻觉问题
### 幻觉一定是有害的吗？
- 在创意写作、故事生成方面，幻觉可以带来新颖有趣的内容。
- 但在事实性任务（法律、医学、科研）中，幻觉是严重问题，必须减少或控制。

# LLMs对比篇
## 目前大模型常见的 base 模型训练和 chat 模型训练方式的区别么？
- base：大语言模型最原始的形态，通常是经过海量文本数据训练的无监督模型。
- chat：在 Base 模型的基础上，针对对话和人机交互进行优化的模型。
- instruction：在 Base 模型基础上，通过监督学习特别优化以理解和执行自然语言指令的模型。
## llama、baichuan和qwen等开源大模型技术对比
### llama2
- tokenizer：BPE，32k
- arch：旋转位置编码，SwiGLU,RMSNorm
- content长度：4096
### mistral 7B
- grouped-query attention：让多个 Query 共享同一个 Key-Value（KV）对，从而减少计算需求。
- Sliding Window Attention：只计算每个 token 在其 512 长度的窗口范围内的注意力

### Qwen
标准transformer架构，中文语料为主
- tokenizer：BPE,152K,编码压缩率低
- arch：旋转位置编码，使用fp32精度的逆频率矩阵，SwiGLU,RMSNorm
- content：在QKV注意力层中添加bias以增强模型的外推能力

### deepseek
采用混合专家架构，动态激活部分参数，显著降低计算成本

## 相比较于llama而言，llama2有哪些改进，对于llama2是应该如何finetune？
- 使用了更大规模的数据集和更广泛的多语种支持
- 低精度训练和量化支持
- 对RLHF进行了更好的支持
## gpt经验篇
### 讲讲Bart和Bert的区别？
- bert：主要用于编码任务，如文本分类等
- bart：用于编码-解码任务，如文本摘要，结合了bert和gpt的特点
### gpt3和gpt2的区别？
- gpt3有175B的参数，远大于gpt2（1.5B）
- gpt3引入了稀疏激活技术

### 编码任务，解码任务，编码解码任务
- 解码任务（GPT） 不是完全不理解输入，而是以自回归方式逐步理解和生成。
- 编码任务（BERT） 是一次性构建整个输入的全局表示。
- 编码-解码任务（BART、T5） 先全局理解输入，再逐步生成输出。

# MOE篇
## MOE篇
### 为什么需要 MOE（Mixture-of-Experts）？
模型规模的扩展会导致训练成本显著增加，计算资源的限制成为了大规模密集模型训练的瓶颈。
### MOE（Mixture-of-Experts）的思路是什么样的？
将大模型拆分成多个小模型(专家，expert)， 每轮迭代根据样本决定激活一部分专家用于计算，达到了节省计算资源的效果；并引入可训练并确保稀疏性的门(gate)机制，以保证计算能力的优化。

### MOE + 数据并行?
数据并行的模式下包含MOE架构，门网络(gate)和专家网络都被复制地放置在各个运算单元上。对现有的代码入侵性较小，但是专家的数量收到单个计算单元的内存大小限制。

### MOE+模型并行？
该策略门网络依然是复制地被放置在每个计算单元上， 但是专家网络被独立地分别放置在各个计算单元上。因此，需引入额外的通信操作，该策略可以允许更多的专家网络们同时被训练，而其数量限制与计算单元的数量(如：GPU数量)是正相关的。会引入额外的通信，因此，相较于数据并行+MOE策略，侵入性更强。

### MOE大模型的优势
- 计算效率高
- 扩展性强，灵活性高
- 参数利用率高

### MOE大模型的缺点
- 专家激活不均衡
- 训练复杂
- 专家管理难

# 大模型蒸馏篇
## 大模型蒸馏篇
### 知识蒸馏 vs. 无监督样本训练
- 知识蒸馏：用大模型（Teacher）指导小模型（Student）学习，提高小模型的性能。
- 无监督样本训练：使用未标注数据，通过自监督或半监督学习提升模型能力，不依赖人工标注数据。

### 量化精度对比总结

| 量化精度 | 计算类型 | 优势 | 劣势 |
|----------|--------|------|------|
| FP16  | 浮点 | 计算加速，精度高 | 仍占较多显存 |
| BF16  | 浮点 | 更稳定，适合训练 | 硬件支持有限 |
| INT8  | 整数 | 高效推理，存储优化 | 精度损失较大 |
| INT4  | 整数 | 超低存储需求 | 需要蒸馏优化 |
| FP8   | 浮点 | 训练+推理均有优化 | 生态支持少 |

### 混合精度原理
混合精度训练是一种同时使用 FP16 和 FP32计算的方法，以减少显存占用并加速计算，同时保持数值稳定性。  
- 权重 & 计算：使用 FP16 存储和计算，提高计算速度并减少显存占用。
- 损失缩放（Loss Scaling）：由于 FP16 可能导致梯度溢出或下溢，通过损失缩放（Loss Scaling）技术，将损失值放大后再进行计算，避免信息丢失。
- 参数（Master Weights）：主权重参数仍然使用 FP32 存储，以确保训练精度稳定。

### BF16 和 FP16 的区别与关系

1. 区别
- BF16 具有 8-bit 指数位，FP16 只有 5-bit 指数位，因此 BF16 的动态范围更大，接近 FP32，而 FP16 容易溢出。
- FP16 具有 10-bit 尾数位，BF16 只有 7-bit 尾数位，因此 FP16 计算精度更高，而 BF16 精度较低。

2. 关系
- 都是 16-bit 浮点数格式，主要用于深度学习加速计算。
- BF16 适用于训练，能提供较大的数值范围，减少梯度爆炸和溢出问题。
- FP16 适用于推理，计算精度较高，但指数范围较小，可能会导致数值下溢或溢出。

